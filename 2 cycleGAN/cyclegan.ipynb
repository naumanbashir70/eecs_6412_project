{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBZY4wzGluPY",
        "outputId": "03e5cd7a-9bba-4aaa-c9b1-b7498f59529b"
      },
      "outputs": [],
      "source": [
        "!conda activate nauman_gpu"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Requiremements**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# conda install pytorch=1.12.0 torchvision=0.13.0 cudatoolkit=11.3.1 -c pytorch\n",
        "# conda install albumentations=1.2.1 -c conda-forge\n",
        "# conda install einops=0.4.1 -c conda-forge\n",
        "# conda install torchsummary=1.5.1 -c ravelbio\n",
        "# conda install tqdm=4.64.0\n",
        "# conda install scikit-image=0.18.3\n",
        "# conda install scikit-learn=1.1.1\n",
        "# conda install torchmetrics=0.9.3 -c conda-forge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Downloading**\n",
        "For training and evaluation we used the BraTS Dataset from RSNA-ASNR-MICCAI BraTS Continuous Evaluation Challenge:\n",
        "https://www.synapse.org/#!Synapse:syn51156910/wiki/622351\n",
        "\n",
        "1. Download the data\n",
        "2. Convert 3d .nii.gz file to 2d\n",
        "    2a. We used two modalities. i) T1 ii) T1 Contrast\n",
        "3. Resize the images to 256 256, (Original images in 3D are 240 240 155)\n",
        "4. We have placed the covnerted 2d resized files here:\n",
        "\n",
        "https://drive.google.com/file/d/1hXJ8CP6BgJz2bFM1OF2LmGtc3VvRgZhS/view?usp=sharing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Preprocessing**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import nibabel as nib\n",
        "import itertools\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converting T1 3D Nifti images to 2D PNG images \n",
        "\n",
        "folder = '/local/data0/home/naumanb/Codes/4_SwinUnetR_BraTS_2023/BraTS2023/TrainingData'\n",
        "\n",
        "for root, dir, files in os.walk(folder):\n",
        "    for file in files:\n",
        "        im = os.path.join(root, file)\n",
        "        # print(file)\n",
        "\n",
        "        # if file.endswith('t1c.nii.gz'):\n",
        "        if file.endswith('t1n.nii.gz'):\n",
        "            print(file)\n",
        "            img = os.path.join(root, file)\n",
        "            img = nib.load(img)\n",
        "            img = img.get_data().astype(np.float32)\n",
        "            img_data = np.rot90(img, 3)\n",
        "\n",
        "            plt.imshow(img_data[:, :, 77], cmap='gray')\n",
        "            plt.axis('off')\n",
        "            # Save as PNG\n",
        "            plt.savefig('/local/data0/home/naumanb/Codes/8_Cx_GAN/data/t1n/' +\n",
        "                        file+'.png', bbox_inches='tight', pad_inches=0)\n",
        "\n",
        "            # Show the plot (optional)\n",
        "            # plt.show()\n",
        "\n",
        "    # break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converting T1-Contrat 3D Nifti images to 2D PNG images\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from skimage.transform import resize\n",
        "import nibabel as nib\n",
        "\n",
        "folder = '/local/data0/home/naumanb/Codes/4_SwinUnetR_BraTS_2023/BraTS2023/TrainingData'\n",
        "\n",
        "for root, dir, files in os.walk(folder):\n",
        "    for file in files:\n",
        "        im = os.path.join(root, file)\n",
        "\n",
        "        if file.endswith('t1c.nii.gz'):\n",
        "            print(file)\n",
        "            img_path = os.path.join(root, file)\n",
        "            img = nib.load(img_path)\n",
        "            img_data = img.get_data().astype(np.float32)\n",
        "            img_data = np.rot90(img_data, 3)\n",
        "\n",
        "\n",
        "            plt.imshow(img_data[:, :, 77], cmap='gray')\n",
        "            plt.axis('off')\n",
        "\n",
        "            # Save as PNG\n",
        "            output_path = '/local/data0/home/naumanb/Codes/8_Cx_GAN/data/t1c/' + file + '.png'\n",
        "            plt.savefig(output_path, bbox_inches='tight', pad_inches=0)\n",
        "            # plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resizing the images to 256x256\n",
        "\n",
        "#Resize the image to 256,256\n",
        "from PIL import Image\n",
        "import os, sys\n",
        "\n",
        "path = \"/local/data0/home/naumanb/Codes/8_Cx_GAN/data/t1c/\"\n",
        "dirs = os.listdir( path )\n",
        "out = \"/local/data0/home/naumanb/Codes/8_Cx_GAN/data/t1c_r/\"\n",
        "\n",
        "def resize():\n",
        "    for item in dirs:\n",
        "        if os.path.isfile(path+item):\n",
        "            im = Image.open(path+item)\n",
        "            f, e = os.path.splitext(path+item)\n",
        "            print(item)\n",
        "            print(f)\n",
        "            imResize = im.resize((256,256), Image.ANTIALIAS)\n",
        "            imResize.save(out+item+'.png', 'PNG', quality=100)\n",
        "\n",
        "resize()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Data Loader**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 13:15:42.945429: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "/local/data0/software/miniconda3/envs/nauman_gpu/lib/python3.9/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.7.0 and strictly below 2.10.0 (nightly versions are not supported). \n",
            " The versions of TensorFlow you are currently using is 2.4.1 and is not supported. \n",
            "Some things might work, some things might not.\n",
            "If you were to encounter a bug, do not file an issue.\n",
            "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
            "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
            "https://github.com/tensorflow/addons\n",
            "  warnings.warn(\n",
            "2023-12-18 13:15:44.866949: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2023-12-18 13:15:44.867735: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2023-12-18 13:15:44.892240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:19:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
            "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
            "2023-12-18 13:15:44.892527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 1 with properties: \n",
            "pciBusID: 0000:1a:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
            "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
            "2023-12-18 13:15:44.892812: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 2 with properties: \n",
            "pciBusID: 0000:67:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
            "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
            "2023-12-18 13:15:44.893100: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 3 with properties: \n",
            "pciBusID: 0000:68:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
            "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
            "2023-12-18 13:15:44.893114: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2023-12-18 13:15:44.895832: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2023-12-18 13:15:44.895877: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2023-12-18 13:15:44.897483: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-12-18 13:15:44.897961: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-12-18 13:15:44.899686: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2023-12-18 13:15:44.900717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2023-12-18 13:15:44.904144: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-12-18 13:15:44.906214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0, 1, 2, 3\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import tf2lib as tl\n",
        "\n",
        "\n",
        "def make_dataset(img_paths, batch_size, load_size, crop_size, training, drop_remainder=True, shuffle=False, repeat=1):\n",
        "    if training:\n",
        "        @tf.function\n",
        "        def _map_fn(img):  # preprocessing\n",
        "            img = tf.image.random_flip_left_right(img)\n",
        "            img = tf.image.resize(img, [load_size, load_size])\n",
        "            img = tf.image.random_crop(img, [crop_size, crop_size, tf.shape(img)[-1]])\n",
        "            img = tf.clip_by_value(img, 0, 255) / 255.0  # or img = tl.minmax_norm(img)\n",
        "            img = img * 2 - 1\n",
        "            return img\n",
        "    else:\n",
        "        @tf.function\n",
        "        def _map_fn(img):  # preprocessing\n",
        "            img = tf.image.resize(img, [crop_size, crop_size])  # or img = tf.image.resize(img, [load_size, load_size]); img = tl.center_crop(img, crop_size)\n",
        "            img = tf.clip_by_value(img, 0, 255) / 255.0  # or img = tl.minmax_norm(img)\n",
        "            img = img * 2 - 1\n",
        "            return img\n",
        "\n",
        "    return tl.disk_image_batch_dataset(img_paths,\n",
        "                                       batch_size,\n",
        "                                       drop_remainder=drop_remainder,\n",
        "                                       map_fn=_map_fn,\n",
        "                                       shuffle=shuffle,\n",
        "                                       repeat=repeat)\n",
        "\n",
        "\n",
        "def make_zip_dataset(A_img_paths, B_img_paths, batch_size, load_size, crop_size, training, shuffle=False, repeat=False):\n",
        "    # zip two datasets aligned by the longer one\n",
        "    if repeat:\n",
        "        A_repeat = B_repeat = None  # cycle both\n",
        "    else:\n",
        "        if len(A_img_paths) >= len(B_img_paths):\n",
        "            A_repeat = 1\n",
        "            B_repeat = None  # cycle the shorter one\n",
        "        else:\n",
        "            A_repeat = None  # cycle the shorter one\n",
        "            B_repeat = 1\n",
        "\n",
        "    A_dataset = make_dataset(A_img_paths, batch_size, load_size, crop_size, training, drop_remainder=True, shuffle=shuffle, repeat=A_repeat)\n",
        "    B_dataset = make_dataset(B_img_paths, batch_size, load_size, crop_size, training, drop_remainder=True, shuffle=shuffle, repeat=B_repeat)\n",
        "\n",
        "    A_B_dataset = tf.data.Dataset.zip((A_dataset, B_dataset))\n",
        "    len_dataset = max(len(A_img_paths), len(B_img_paths)) // batch_size\n",
        "\n",
        "    return A_B_dataset, len_dataset\n",
        "\n",
        "\n",
        "class ItemPool:\n",
        "\n",
        "    def __init__(self, pool_size=50):\n",
        "        self.pool_size = pool_size\n",
        "        self.items = []\n",
        "\n",
        "    def __call__(self, in_items):\n",
        "        # `in_items` should be a batch tensor\n",
        "\n",
        "        if self.pool_size == 0:\n",
        "            return in_items\n",
        "\n",
        "        out_items = []\n",
        "        for in_item in in_items:\n",
        "            if len(self.items) < self.pool_size:\n",
        "                self.items.append(in_item)\n",
        "                out_items.append(in_item)\n",
        "            else:\n",
        "                if np.random.rand() > 0.5:\n",
        "                    idx = np.random.randint(0, len(self.items))\n",
        "                    out_item, self.items[idx] = self.items[idx], in_item\n",
        "                    out_items.append(out_item)\n",
        "                else:\n",
        "                    out_items.append(in_item)\n",
        "        return tf.stack(out_items, axis=0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Network**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Selected GPU: /device:GPU:0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 13:15:46.748934: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-12-18 13:15:46.750792: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2023-12-18 13:15:46.751360: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:68:00.0 name: NVIDIA GeForce RTX 2080 Ti computeCapability: 7.5\n",
            "coreClock: 1.545GHz coreCount: 68 deviceMemorySize: 10.75GiB deviceMemoryBandwidth: 573.69GiB/s\n",
            "2023-12-18 13:15:46.751404: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2023-12-18 13:15:46.751434: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n",
            "2023-12-18 13:15:46.751443: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.10\n",
            "2023-12-18 13:15:46.751452: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2023-12-18 13:15:46.751461: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2023-12-18 13:15:46.751471: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2023-12-18 13:15:46.751480: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.10\n",
            "2023-12-18 13:15:46.751490: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-12-18 13:15:46.752146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 3\n",
            "2023-12-18 13:15:46.752188: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.10.1\n",
            "2023-12-18 13:15:47.234789: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2023-12-18 13:15:47.234818: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      3 \n",
            "2023-12-18 13:15:47.234825: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 3:   N \n",
            "2023-12-18 13:15:47.235785: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 9799 MB memory) -> physical GPU (device: 3, name: NVIDIA GeForce RTX 2080 Ti, pci bus id: 0000:68:00.0, compute capability: 7.5)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "    try:\n",
        "        tf.config.experimental.set_visible_devices(gpus[3], 'GPU')\n",
        "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "        print(f\"Selected GPU: {logical_gpus[0].name}\")\n",
        "    except RuntimeError as e:\n",
        "        print(e)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_addons as tfa\n",
        "import tensorflow.keras as keras\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                  networks                                  =\n",
        "# ==============================================================================\n",
        "\n",
        "def _get_norm_layer(norm):\n",
        "    if norm == 'none':\n",
        "        return lambda: lambda x: x\n",
        "    elif norm == 'batch_norm':\n",
        "        return keras.layers.BatchNormalization\n",
        "    elif norm == 'instance_norm':\n",
        "        return tfa.layers.InstanceNormalization\n",
        "    elif norm == 'layer_norm':\n",
        "        return keras.layers.LayerNormalization\n",
        "\n",
        "\n",
        "def ResnetGenerator(input_shape=(256, 256, 3),\n",
        "                    output_channels=3,\n",
        "                    dim=64,\n",
        "                    n_downsamplings=2,\n",
        "                    n_blocks=9,\n",
        "                    norm='instance_norm'):\n",
        "    Norm = _get_norm_layer(norm)\n",
        "\n",
        "    def _residual_block(x):\n",
        "        dim = x.shape[-1]\n",
        "        h = x\n",
        "\n",
        "        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "        h = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n",
        "        h = Norm()(h)\n",
        "        h = tf.nn.relu(h)\n",
        "\n",
        "        h = tf.pad(h, [[0, 0], [1, 1], [1, 1], [0, 0]], mode='REFLECT')\n",
        "        h = keras.layers.Conv2D(dim, 3, padding='valid', use_bias=False)(h)\n",
        "        h = Norm()(h)\n",
        "\n",
        "        return keras.layers.add([x, h])\n",
        "\n",
        "    # 0\n",
        "    h = inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # 1\n",
        "    h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
        "    h = keras.layers.Conv2D(dim, 7, padding='valid', use_bias=False)(h)\n",
        "    h = Norm()(h)\n",
        "    h = tf.nn.relu(h)\n",
        "\n",
        "    # 2\n",
        "    for _ in range(n_downsamplings):\n",
        "        dim *= 2\n",
        "        h = keras.layers.Conv2D(dim, 3, strides=2, padding='same', use_bias=False)(h)\n",
        "        h = Norm()(h)\n",
        "        h = tf.nn.relu(h)\n",
        "\n",
        "    # 3\n",
        "    for _ in range(n_blocks):\n",
        "        h = _residual_block(h)\n",
        "\n",
        "    # 4\n",
        "    for _ in range(n_downsamplings):\n",
        "        dim //= 2\n",
        "        h = keras.layers.Conv2DTranspose(dim, 3, strides=2, padding='same', use_bias=False)(h)\n",
        "        h = Norm()(h)\n",
        "        h = tf.nn.relu(h)\n",
        "\n",
        "    # 5\n",
        "    h = tf.pad(h, [[0, 0], [3, 3], [3, 3], [0, 0]], mode='REFLECT')\n",
        "    h = keras.layers.Conv2D(output_channels, 7, padding='valid')(h)\n",
        "    h = tf.tanh(h)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=h)\n",
        "\n",
        "\n",
        "def ConvDiscriminator(input_shape=(256, 256, 3),\n",
        "                      dim=64,\n",
        "                      n_downsamplings=3,\n",
        "                      norm='instance_norm'):\n",
        "    dim_ = dim\n",
        "    Norm = _get_norm_layer(norm)\n",
        "\n",
        "    # 0\n",
        "    h = inputs = keras.Input(shape=input_shape)\n",
        "\n",
        "    # 1\n",
        "    h = keras.layers.Conv2D(dim, 4, strides=2, padding='same')(h)\n",
        "    h = tf.nn.leaky_relu(h, alpha=0.2)\n",
        "\n",
        "    for _ in range(n_downsamplings - 1):\n",
        "        dim = min(dim * 2, dim_ * 8)\n",
        "        h = keras.layers.Conv2D(dim, 4, strides=2, padding='same', use_bias=False)(h)\n",
        "        h = Norm()(h)\n",
        "        h = tf.nn.leaky_relu(h, alpha=0.2)\n",
        "\n",
        "    # 2\n",
        "    dim = min(dim * 2, dim_ * 8)\n",
        "    h = keras.layers.Conv2D(dim, 4, strides=1, padding='same', use_bias=False)(h)\n",
        "    h = Norm()(h)\n",
        "    h = tf.nn.leaky_relu(h, alpha=0.2)\n",
        "\n",
        "    # 3\n",
        "    h = keras.layers.Conv2D(1, 4, strides=1, padding='same')(h)\n",
        "\n",
        "    return keras.Model(inputs=inputs, outputs=h)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                          learning rate scheduler                           =\n",
        "# ==============================================================================\n",
        "\n",
        "class LinearDecay(keras.optimizers.schedules.LearningRateSchedule):\n",
        "    # if `step` < `step_decay`: use fixed learning rate\n",
        "    # else: linearly decay the learning rate to zero\n",
        "\n",
        "    def __init__(self, initial_learning_rate, total_steps, step_decay):\n",
        "        super(LinearDecay, self).__init__()\n",
        "        self._initial_learning_rate = initial_learning_rate\n",
        "        self._steps = total_steps\n",
        "        self._step_decay = step_decay\n",
        "        self.current_learning_rate = tf.Variable(initial_value=initial_learning_rate, trainable=False, dtype=tf.float32)\n",
        "\n",
        "    def __call__(self, step):\n",
        "        self.current_learning_rate.assign(tf.cond(\n",
        "            step >= self._step_decay,\n",
        "            true_fn=lambda: self._initial_learning_rate * (1 - 1 / (self._steps - self._step_decay) * (step - self._step_decay)),\n",
        "            false_fn=lambda: self._initial_learning_rate\n",
        "        ))\n",
        "        return self.current_learning_rate\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Training**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function make_dataset.<locals>._map_fn at 0x7fbeff65dc10> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function make_dataset.<locals>._map_fn at 0x7fbeff65dc10> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <function make_dataset.<locals>._map_fn at 0x7fbdc48970d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function make_dataset.<locals>._map_fn at 0x7fbdc48970d0> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING:tensorflow:AutoGraph could not transform <bound method GroupNormalization.call of <tensorflow_addons.layers.normalizations.InstanceNormalization object at 0x7fbdb6eb6d90>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <bound method GroupNormalization.call of <tensorflow_addons.layers.normalizations.InstanceNormalization object at 0x7fbdb6eb6d90>> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-12-18 13:16:38.484353: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2023-12-18 13:16:38.496116: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 3300000000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "No checkpoint specified (save_path=None); nothing is being restored.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch Loop:   0%|          | 0/100 [00:00<?, ?it/s]2023-12-18 13:16:53.172102: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.7\n",
            "2023-12-18 13:16:54.700088: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:AutoGraph could not transform <function gradient_penalty at 0x7fbddc057280> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
            "WARNING: AutoGraph could not transform <function gradient_penalty at 0x7fbddc057280> and will run it as-is.\n",
            "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
            "Cause: module 'gast' has no attribute 'Index'\n",
            "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:45<00:00,  3.51it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:33<00:00,  3.66it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:37<00:00,  3.61it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.70it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.70it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.70it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:43<00:00,  3.52it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.70it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.70it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.70it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:32<00:00,  3.67it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:34<00:00,  3.65it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.68it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:30<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Inner Epoch Loop: 100%|██████████| 1000/1000 [04:31<00:00,  3.69it/s]\n",
            "Epoch Loop: 100%|██████████| 100/100 [7:34:25<00:00, 272.65s/it]\n"
          ]
        }
      ],
      "source": [
        "import functools\n",
        "import imlib as im\n",
        "import numpy as np\n",
        "import pylib as py\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras as keras\n",
        "import tf2lib as tl\n",
        "import tf2gan as gan\n",
        "import tqdm\n",
        "import glob\n",
        "import os\n",
        "from PIL import Image\n",
        "import tensorflow as tf\n",
        "import data\n",
        "# import module\n",
        "import warnings\n",
        "\n",
        "# Disable all warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import os\n",
        "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                   param                                    =\n",
        "# ==============================================================================\n",
        "\n",
        "dataset = 'brats'\n",
        "datasets_dir = 'datasets'\n",
        "load_size = 256\n",
        "crop_size = 256\n",
        "batch_size = 1\n",
        "epochs = 100\n",
        "epoch_decay = 50\n",
        "lr = 0.0002\n",
        "beta_1 = 0.5\n",
        "adversarial_loss_mode = 'lsgan'\n",
        "gradient_penalty_mode = 'none'\n",
        "gradient_penalty_weight = 10.0\n",
        "cycle_loss_weight = 10.0\n",
        "identity_loss_weight = 0.0\n",
        "pool_size = 50\n",
        "\n",
        "\n",
        "\n",
        "# output_dir\n",
        "output_dir = py.join('output_brats', dataset)\n",
        "py.mkdir(output_dir)\n",
        "\n",
        "# save settings\n",
        "# save settings\n",
        "settings = {\n",
        "    'dataset': dataset,\n",
        "    'datasets_dir': datasets_dir,\n",
        "    'load_size': load_size,\n",
        "    'crop_size': crop_size,\n",
        "    'batch_size': batch_size,\n",
        "    'epochs': epochs,\n",
        "    'epoch_decay': epoch_decay,\n",
        "    'lr': lr,\n",
        "    'beta_1': beta_1,\n",
        "    'adversarial_loss_mode': adversarial_loss_mode,\n",
        "    'gradient_penalty_mode': gradient_penalty_mode,\n",
        "    'gradient_penalty_weight': gradient_penalty_weight,\n",
        "    'cycle_loss_weight': cycle_loss_weight,\n",
        "    'identity_loss_weight': identity_loss_weight,\n",
        "    'pool_size': pool_size,\n",
        "}\n",
        "\n",
        "py.args_to_yaml(py.join(output_dir, 'settings.yml'), type('', (), settings)())\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                    data                                    =\n",
        "# ==============================================================================\n",
        "\n",
        "\n",
        "#-------------------------------------------- old Unpaired dataset --------------------------------------------#\n",
        "A_img_paths = py.glob(py.join('datasets/train_folder', 't1n_r'), '*.png')\n",
        "B_img_paths = py.glob(py.join('datasets/train_folder', 't1c_r'), '*.png')\n",
        "A_B_dataset, len_dataset = data.make_zip_dataset(A_img_paths, B_img_paths, batch_size, load_size, crop_size, training=True, repeat=False)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "A2B_pool = data.ItemPool(pool_size)\n",
        "B2A_pool = data.ItemPool(pool_size)\n",
        "\n",
        "A_img_paths_test = py.glob(py.join('datasets/test_folder/', 't1n_r'), '*.png')\n",
        "B_img_paths_test = py.glob(py.join('datasets/test_folder/', 't1c_r'), '*.png')\n",
        "A_B_dataset_test, _ = data.make_zip_dataset(A_img_paths_test, B_img_paths_test, batch_size, load_size, crop_size, training=False, repeat=True)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                   models                                   =\n",
        "# ==============================================================================\n",
        "\n",
        "G_A2B = ResnetGenerator(input_shape=(crop_size, crop_size, 3))\n",
        "G_B2A = ResnetGenerator(input_shape=(crop_size, crop_size, 3))\n",
        "\n",
        "D_A = ConvDiscriminator(input_shape=(crop_size, crop_size, 3))\n",
        "D_B = ConvDiscriminator(input_shape=(crop_size, crop_size, 3))\n",
        "\n",
        "d_loss_fn, g_loss_fn = gan.get_adversarial_losses_fn(adversarial_loss_mode)\n",
        "cycle_loss_fn = tf.losses.MeanAbsoluteError()\n",
        "identity_loss_fn = tf.losses.MeanAbsoluteError()\n",
        "\n",
        "G_lr_scheduler = LinearDecay(lr, epochs * len_dataset, epoch_decay * len_dataset)\n",
        "D_lr_scheduler = LinearDecay(lr, epochs * len_dataset, epoch_decay * len_dataset)\n",
        "G_optimizer = keras.optimizers.Adam(learning_rate=G_lr_scheduler, beta_1=beta_1)\n",
        "D_optimizer = keras.optimizers.Adam(learning_rate=D_lr_scheduler, beta_1=beta_1)\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                 train step                                 =\n",
        "# ==============================================================================\n",
        "\n",
        "@tf.function\n",
        "def train_G(A, B):\n",
        "    with tf.GradientTape() as t:\n",
        "        A2B = G_A2B(A, training=True)\n",
        "        B2A = G_B2A(B, training=True)\n",
        "        A2B2A = G_B2A(A2B, training=True)\n",
        "        B2A2B = G_A2B(B2A, training=True)\n",
        "        A2A = G_B2A(A, training=True)\n",
        "        B2B = G_A2B(B, training=True)\n",
        "\n",
        "        A2B_d_logits = D_B(A2B, training=True)\n",
        "        B2A_d_logits = D_A(B2A, training=True)\n",
        "\n",
        "        A2B_g_loss = g_loss_fn(A2B_d_logits)\n",
        "        B2A_g_loss = g_loss_fn(B2A_d_logits)\n",
        "        A2B2A_cycle_loss = cycle_loss_fn(A, A2B2A)\n",
        "        B2A2B_cycle_loss = cycle_loss_fn(B, B2A2B)\n",
        "        A2A_id_loss = identity_loss_fn(A, A2A)\n",
        "        B2B_id_loss = identity_loss_fn(B, B2B)\n",
        "\n",
        "        G_loss = (A2B_g_loss + B2A_g_loss) + (A2B2A_cycle_loss + B2A2B_cycle_loss) * cycle_loss_weight + (A2A_id_loss + B2B_id_loss) * identity_loss_weight\n",
        "\n",
        "    G_grad = t.gradient(G_loss, G_A2B.trainable_variables + G_B2A.trainable_variables)\n",
        "    G_optimizer.apply_gradients(zip(G_grad, G_A2B.trainable_variables + G_B2A.trainable_variables))\n",
        "\n",
        "    return A2B, B2A, {'A2B_g_loss': A2B_g_loss,\n",
        "                      'B2A_g_loss': B2A_g_loss,\n",
        "                      'A2B2A_cycle_loss': A2B2A_cycle_loss,\n",
        "                      'B2A2B_cycle_loss': B2A2B_cycle_loss,\n",
        "                      'A2A_id_loss': A2A_id_loss,\n",
        "                      'B2B_id_loss': B2B_id_loss}\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def train_D(A, B, A2B, B2A):\n",
        "    with tf.GradientTape() as t:\n",
        "        A_d_logits = D_A(A, training=True)\n",
        "        B2A_d_logits = D_A(B2A, training=True)\n",
        "        B_d_logits = D_B(B, training=True)\n",
        "        A2B_d_logits = D_B(A2B, training=True)\n",
        "\n",
        "        A_d_loss, B2A_d_loss = d_loss_fn(A_d_logits, B2A_d_logits)\n",
        "        B_d_loss, A2B_d_loss = d_loss_fn(B_d_logits, A2B_d_logits)\n",
        "        D_A_gp = gan.gradient_penalty(functools.partial(D_A, training=True), A, B2A, mode=gradient_penalty_mode)\n",
        "        D_B_gp = gan.gradient_penalty(functools.partial(D_B, training=True), B, A2B, mode=gradient_penalty_mode)\n",
        "\n",
        "        D_loss = (A_d_loss + B2A_d_loss) + (B_d_loss + A2B_d_loss) + (D_A_gp + D_B_gp) * gradient_penalty_weight\n",
        "\n",
        "    D_grad = t.gradient(D_loss, D_A.trainable_variables + D_B.trainable_variables)\n",
        "    D_optimizer.apply_gradients(zip(D_grad, D_A.trainable_variables + D_B.trainable_variables))\n",
        "\n",
        "    return {'A_d_loss': A_d_loss + B2A_d_loss,\n",
        "            'B_d_loss': B_d_loss + A2B_d_loss,\n",
        "            'D_A_gp': D_A_gp,\n",
        "            'D_B_gp': D_B_gp}\n",
        "\n",
        "\n",
        "def train_step(A, B):\n",
        "    A2B, B2A, G_loss_dict = train_G(A, B)\n",
        "\n",
        "    # cannot autograph `A2B_pool`\n",
        "    A2B = A2B_pool(A2B)  # or A2B = A2B_pool(A2B.numpy()), but it is much slower\n",
        "    B2A = B2A_pool(B2A)  # because of the communication between CPU and GPU\n",
        "\n",
        "    D_loss_dict = train_D(A, B, A2B, B2A)\n",
        "\n",
        "    return G_loss_dict, D_loss_dict\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def sample(A, B):\n",
        "    A2B = G_A2B(A, training=False)\n",
        "    B2A = G_B2A(B, training=False)\n",
        "    A2B2A = G_B2A(A2B, training=False)\n",
        "    B2A2B = G_A2B(B2A, training=False)\n",
        "    return A2B, B2A, A2B2A, B2A2B\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                    run                                     =\n",
        "# ==============================================================================\n",
        "\n",
        "# epoch counter\n",
        "ep_cnt = tf.Variable(initial_value=0, trainable=False, dtype=tf.int64)\n",
        "\n",
        "# checkpoint\n",
        "checkpoint = tl.Checkpoint(dict(G_A2B=G_A2B,\n",
        "                                G_B2A=G_B2A,\n",
        "                                D_A=D_A,\n",
        "                                D_B=D_B,\n",
        "                                G_optimizer=G_optimizer,\n",
        "                                D_optimizer=D_optimizer,\n",
        "                                ep_cnt=ep_cnt),\n",
        "                           py.join(output_dir, 'checkpoints'),\n",
        "                           max_to_keep=5)\n",
        "try:  # restore checkpoint including the epoch counter\n",
        "    checkpoint.restore().assert_existing_objects_matched()\n",
        "except Exception as e:\n",
        "    print(e)\n",
        "\n",
        "# summary\n",
        "train_summary_writer = tf.summary.create_file_writer(py.join(output_dir, 'summaries', 'train'))\n",
        "\n",
        "# sample\n",
        "test_iter = iter(A_B_dataset_test)\n",
        "sample_dir = py.join(output_dir, 'samples_training')\n",
        "py.mkdir(sample_dir)\n",
        "\n",
        "# main loop\n",
        "with train_summary_writer.as_default():\n",
        "    for ep in tqdm.trange(epochs, desc='Epoch Loop'):\n",
        "        if ep < ep_cnt:\n",
        "            continue\n",
        "\n",
        "        # update epoch counter\n",
        "        ep_cnt.assign_add(1)\n",
        "\n",
        "        # train for an epoch\n",
        "        for A, B in tqdm.tqdm(A_B_dataset, desc='Inner Epoch Loop', total=len_dataset):\n",
        "            G_loss_dict, D_loss_dict = train_step(A, B)\n",
        "\n",
        "            # # summary\n",
        "            tl.summary(G_loss_dict, step=G_optimizer.iterations, name='G_losses')\n",
        "            tl.summary(D_loss_dict, step=G_optimizer.iterations, name='D_losses')\n",
        "            tl.summary({'learning rate': G_lr_scheduler.current_learning_rate}, step=G_optimizer.iterations, name='learning rate')\n",
        "\n",
        "            # sample\n",
        "            if G_optimizer.iterations.numpy() % 100 == 0:\n",
        "                A, B = next(test_iter)\n",
        "                A2B, B2A, A2B2A, B2A2B = sample(A, B)\n",
        "                img = im.immerge(np.concatenate([A, A2B, A2B2A, B, B2A, B2A2B], axis=0), n_rows=2)\n",
        "                im.imwrite(img, py.join(sample_dir, 'iter-%09d.jpg' % G_optimizer.iterations.numpy()))\n",
        "\n",
        "        # save checkpoint\n",
        "        checkpoint.save(ep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Test**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import imlib as im\n",
        "import numpy as np\n",
        "import pylib as py\n",
        "import tensorflow as tf\n",
        "import tf2lib as tl\n",
        "\n",
        "import data\n",
        "import module\n",
        "import numpy as np\n",
        "import imageio as iio\n",
        "import os.path as py\n",
        "# Replace these lines\n",
        "import pylib as py\n",
        "import os.path as py\n",
        "\n",
        "# With something like this\n",
        "import pylib as py\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                   param                                    =\n",
        "# ==============================================================================\n",
        "\n",
        "# py.arg('--experiment_dir')\n",
        "# py.arg('--batch_size', type=int, default=32)\n",
        "# test_args = py.args()\n",
        "# args = py.args_from_yaml(py.join(test_args.experiment_dir, 'settings.yml'))\n",
        "# args.__dict__.update(test_args.__dict__)\n",
        "\n",
        "\n",
        "load_size= 286\n",
        "crop_size= 256\n",
        "batch_size= 1\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# =                                    test                                    =\n",
        "# ==============================================================================\n",
        "\n",
        "# data\n",
        "A_img_paths_test = py.glob(py.join('datasets/test_folder', 't1n_r'), '*.png')\n",
        "B_img_paths_test = py.glob(py.join('datasets/test_folder', 't1c_r'), '*.png')\n",
        "\n",
        "A_dataset_test = data.make_dataset(A_img_paths_test,batch_size,load_size, crop_size,\n",
        "                                   training=False, drop_remainder=False, shuffle=False, repeat=1)\n",
        "\n",
        "B_dataset_test = data.make_dataset(B_img_paths_test, batch_size, load_size, crop_size,\n",
        "                                   training=False, drop_remainder=False, shuffle=False, repeat=1)\n",
        "\n",
        "# model\n",
        "G_A2B = module.ResnetGenerator(input_shape=(crop_size, crop_size, 3))\n",
        "G_B2A = module.ResnetGenerator(input_shape=(crop_size, crop_size, 3))\n",
        "\n",
        "# resotre\n",
        "tl.Checkpoint(dict(G_A2B=G_A2B, G_B2A=G_B2A), py.join('output_brats/brats/', 'checkpoints')).restore()\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def sample_A2B(A):\n",
        "    A2B = G_A2B(A, training=False)\n",
        "    A2B2A = G_B2A(A2B, training=False)\n",
        "    return A2B, A2B2A\n",
        "\n",
        "\n",
        "@tf.function\n",
        "def sample_B2A(B):\n",
        "    B2A = G_B2A(B, training=False)\n",
        "    B2A2B = G_A2B(B2A, training=False)\n",
        "    return B2A, B2A2B\n",
        "\n",
        "\n",
        "# run\n",
        "save_dir = py.join('output_brats/brats/', 'samples_testing', 'A2B')\n",
        "py.mkdir(save_dir)\n",
        "i = 0\n",
        "for A in A_dataset_test:\n",
        "    A2B, A2B2A = sample_A2B(A)\n",
        "    for A_i, A2B_i, A2B2A_i in zip(A, A2B, A2B2A):\n",
        "        img = np.concatenate([A_i.numpy(), A2B_i.numpy(), A2B2A_i.numpy()], axis=1)\n",
        "        # im.imwrite(img, py.join(save_dir, py.name_ext(A_img_paths_test[i])))\n",
        "        iio.imsave(py.join(save_dir, py.name_ext(A_img_paths_test[i])), img)\n",
        "        i += 1\n",
        "\n",
        "save_dir = py.join('output_brats/brats/', 'samples_testing', 'B2A')\n",
        "py.mkdir(save_dir)\n",
        "i = 0\n",
        "for B in B_dataset_test:\n",
        "    B2A, B2A2B = sample_B2A(B)\n",
        "    for B_i, B2A_i, B2A2B_i in zip(B, B2A, B2A2B):\n",
        "        img = np.concatenate([B_i.numpy(), B2A_i.numpy(), B2A2B_i.numpy()], axis=1)\n",
        "        # im.imwrite(img, py.join(save_dir, py.name_ext(B_img_paths_test[i])))\n",
        "        iio.imsave(py.join(save_dir, py.name_ext(B_img_paths_test[i])), img)\n",
        "        i += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A2B Metrics:\n",
            "Mean Squared Error: 0.11810231861677417\n",
            "SSIM: 0.6296439\n",
            "PSNR: 9.880537\n",
            "\n",
            "B2A Metrics:\n",
            "Mean Squared Error: 0.006030355766295807\n",
            "SSIM: 0.88735026\n",
            "PSNR: 23.636662\n"
          ]
        }
      ],
      "source": [
        "from skimage.metrics import mean_squared_error, structural_similarity, peak_signal_noise_ratio\n",
        "\n",
        "# Function to preprocess images (if needed)\n",
        "def preprocess_image(image):\n",
        "    # You can add preprocessing steps here if necessary\n",
        "    return image.numpy()\n",
        "\n",
        "# Placeholder arrays for storing metrics\n",
        "mse_a2b_values = []\n",
        "ssim_a2b_values = []\n",
        "psnr_a2b_values = []\n",
        "\n",
        "mse_b2a_values = []\n",
        "ssim_b2a_values = []\n",
        "psnr_b2a_values = []\n",
        "\n",
        "# Calculate metrics for A2B\n",
        "for A in A_dataset_test:\n",
        "    A2B, _ = sample_A2B(A)\n",
        "    for A_i, A2B_i in zip(A, A2B):\n",
        "        A_i = preprocess_image(A_i)\n",
        "        A2B_i = preprocess_image(A2B_i)\n",
        "\n",
        "        mse_value = mean_squared_error(A_i, A2B_i)\n",
        "        ssim_value = structural_similarity(A_i, A2B_i, multichannel=True)\n",
        "        psnr_value = tf.image.psnr(A_i, A2B_i, max_val=1.0)\n",
        "\n",
        "        mse_a2b_values.append(mse_value)\n",
        "        ssim_a2b_values.append(ssim_value)\n",
        "        psnr_a2b_values.append(psnr_value)\n",
        "\n",
        "# Calculate metrics for B2A\n",
        "for B in B_dataset_test:\n",
        "    _, B2A = sample_B2A(B)\n",
        "    for B_i, B2A_i in zip(B, B2A):\n",
        "        B_i = preprocess_image(B_i)\n",
        "        B2A_i = preprocess_image(B2A_i)\n",
        "\n",
        "        mse_value = mean_squared_error(B_i, B2A_i)\n",
        "        ssim_value = structural_similarity(B_i, B2A_i, multichannel=True)\n",
        "        psnr_value = tf.image.psnr(B_i, B2A_i, max_val=1.0)\n",
        "\n",
        "        mse_b2a_values.append(mse_value)\n",
        "        ssim_b2a_values.append(ssim_value)\n",
        "        psnr_b2a_values.append(psnr_value)\n",
        "\n",
        "# Calculate mean values\n",
        "mean_mse_a2b = np.mean(mse_a2b_values)\n",
        "mean_ssim_a2b = np.mean(ssim_a2b_values)\n",
        "mean_psnr_a2b = np.mean(psnr_a2b_values)\n",
        "\n",
        "mean_mse_b2a = np.mean(mse_b2a_values)\n",
        "mean_ssim_b2a = np.mean(ssim_b2a_values)\n",
        "mean_psnr_b2a = np.mean(psnr_b2a_values)\n",
        "\n",
        "# Print results\n",
        "print(\"A2B Metrics:\")\n",
        "print(\"Mean Squared Error:\", mean_mse_a2b)\n",
        "print(\"SSIM:\", mean_ssim_a2b)\n",
        "print(\"PSNR:\", mean_psnr_a2b)\n",
        "\n",
        "print(\"\\nB2A Metrics:\")\n",
        "print(\"Mean Squared Error:\", mean_mse_b2a)\n",
        "print(\"SSIM:\", mean_ssim_b2a)\n",
        "print(\"PSNR:\", mean_psnr_b2a)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
